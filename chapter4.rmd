# Chapter 4 - Clustering and classification


## part 1 
I created a new RMarkdown file and named it chapter4.Rmd  and included it as a child file in my index.Rmd. 

## part 2 
I accessd the Boston data in MASS library. Details about the used data package can be seen here https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html . 
The data describes the Housing Values in Suburbs of Boston. The data has variables like this : crim = per capita crime rate by town, zn = proportion of residential land zoned for lots over 25,000 sq.ft, indus =proportion of non-retail business acres per town. Complete list of variables is shown in the colnames output in below.

```{r}
library(MASS)
data("Boston")
# explore the dataset
colnames(Boston)
summary(Boston)

# plot matrix of the variables
pairs(Boston)

```
## Part 3 
Below I have shown an graphical overview of the variables and summaries of the variables. I have also drawn the correlations between the variables. There seems to be at least strong correlations between crime and taxes, lstat (lower status of the population (percent) ) and zn ( proportion of residential land zoned for lots over 25,000 sq.ft.), nox( nitrogen oxides concentration (parts per 10 million)) and indus ( proportion of non-retail business acres per town), taxes and indus and tax ( ) and  rad (index of accessibility to radial highways). 

The variables do not seem to quite normally distributed. 

```{r}
library(tidyr); library(dplyr); library(ggplot2)

# plots a glimpse of the variables
glimpse(Boston)
# use gather() to gather columns into key-value pairs and then glimpse() at the resulting data
gather(Boston) %>% glimpse

# draw a bar plot of each variable
gather(Boston) %>% ggplot(aes(value)) +geom_bar()+ facet_wrap("key", scales = "free")
# drawing boxplots of each variable
gather(Boston) %>% ggplot(aes(x = value, y = value)) +geom_boxplot()+ facet_wrap("key", scales = "free")
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)

# print the correlation matrix
cor_matrix

```
## part 4
Below code standardises the data and prints out the summaries of the data. Then categorical variables are created of the crime rate in Boston dataset. Then the set is further divided into train and test set so that 80% of the set is in the training set. 

All the data is now divided around zero and there is the new variable crime with the classes: "low","med_low", "med_high", "high".

```{r }

# standardise the variables
boston_scaled <- scale(Boston)

# print out the summaries of the sacaled variables
summary(boston_scaled)

#cheching the classes of the boston_scaled object
class(boston_scaled)

#creating a dataframe out of the boston_scaled object
boston_scaled <- as.data.frame(boston_scaled)
#checking that the transformation worked
is.data.frame(boston_scaled)

#saving the scaled crime rate as scaled_crim
scaled_crim <- boston_scaled$crim

#printing out the summary of scaled crime rate
summary(scaled_crim)

#creating ad quantile vector of crime and then

# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)

#creating a categorical variable "crime"
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low","med_low", "med_high", "high"))

# taking a look at the new factor crime 
table(crime)
bins 

#removing the original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

#adding the new categorical value to the data 
boston_scaled <- data.frame(boston_scaled, crime)


#checking the number of rows in the dataset
n <- nrow(Boston)
#choosing randomly 80% of the rows 
ind <- sample(n, size = n * 0.8)

# creating a training set
train <- boston_scaled[ind,]

#creating the testing set 
test <- boston_scaled[-ind,]

```
All the data is now divided around zero and there is the new variable crime with the classes: "low","med_low", "med_high", "high".


## part 5 
I fitted the linear discriminant analysis on the train set. Then I used the categorical crime rate as the target variable and all other variables as prediction variables. Finally I drew the LDA biplot.  

```{r}
# Doing the discriminant analysis using the crime as the target and all others as predictors
lda.fit <- lda(crime ~., data = train)
#printing the lda.fit obcjet
lda.fit

#Drawing the lda biplot with arrows 
lda.arrows <- function(x, myscale = 10, arrow_heads = 0.1, color = "purple", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0,
         x1= myscale * heads[,choices[1]],
         y1= myscale * heads[,choices[2]], col = color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col = color, pos = 3)
}

# target classes as numeric 
classes <- as.numeric(train$crime)
#plotting the lda
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1 )

```

In the LDA the linear discriminant 1 (LD1)  explain 94% of the variance between groups.  
In the LDA biplot we have the classes in different colour and the variables in the middle. The direction and length of the arrow displays how the variable impacts the model and these are based on the coefficients. So as the Rad variable coefficient is quite big (2.97683720) it is clearly visible in the visualisation as well. 


## part 6 
I saved the crime categories from the test set and removed the categorical crime variable from the test dataset. Then I predicted the classes with the LDA model on the test data. I cross tabulated the results with the crime categories on the test set. 

The prediction seems to be reasonable. The predictions are partly quite accurate at least for high and fairly good for med_low class. For other classes the predictions are not as good with quite a lot of wrong predictions.

```{r}
#saving the correct classes from the test data 
correct_classes <- test$crime
#removing the crime variable from the test data 
test <- dplyr::select(test, -crime)


#predicting classes with the test data 
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulating the results 
table(correct = correct_classes , predicted = lda.pred$class)

```
## part 7

I standardised the Boston dataset and ran the k-means algorithm on the set. I visualised the clusters with the pairs function. It would seem that two clusters would an optimal solution in this case since the drop is the steepest at this point. Based on the pairs visualisation it seems that at least rad and tax are good discriminatory variables.
```{r}
library(MASS)
#accessing the data
data("Boston")
#scaling the data 
boston_scaled <- scale(Boston)
#calculating the euclidian distances of the observations.
dist_eu <- dist(boston_scaled)

#adding the max number of clusters first 15 and then 2 as this was the optimal. 
k_max <- 2
# calculating the total with sum of squares 
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualising the results 
plot(1:k_max, twcss, type='b')

# running the k-means clustering
km <-kmeans(dist_eu, centers = 2)

#plotting the scaled_boston dataset with clusters

pairs(Boston, col = km$cluster)

```

