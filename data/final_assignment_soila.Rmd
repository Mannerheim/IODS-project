# Final Assignment

## Authors infromation:
  ---
title: "Final Assignment"
author: "Tuomas Soila"
date: "6 3 2017"
email: tuomas.soila@helsinki.fi
output: html_document
---

•	Your full name, date and email address at the beginning of the page. Use the yaml header of the RMarkdown document to set these.

•	Brief description of the "research question" you are exploring, possibly including your hypothesis (max 2 points)
## Research question:
This reserach will look into the predictive power of linear discriminant analysis on the Acohol consumption dataset so that grades are the target variable and all other variables act as a predictory variables and after this I will do a regression analysis on with the grades as the target and others as predicatory variables.. My hypothesis is that other aspects such as family life quality of life have an effect on the final grade.  

##part 1 
Reading the file and printing the column names. 
```{r}
alc <- read.table("/Users/Tuomassoila/IODS-project/data/alc_use_final_assignment.txt", sep = "," , header=TRUE)
colnames(alc)
glimpse(alc)

```
## part 2
Description of your data and its variables. Where is the data from, what does it relate to, what do the variables represent, what has been done to the data before analysis? (max 2 points)

The data consist of two separate datasets that have been combined to one. Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) are printed above. It has 35 variables and 382 observations.

Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets: 
1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) 
2 sex - student's sex (binary: 'F' - female or 'M' - male) 
3 age - student's age (numeric: from 15 to 22) 
4 address - student's home address type (binary: 'U' - urban or 'R' - rural) 
5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) 
6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) 
7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 Ã¢â‚¬â€œ 5th to 9th grade, 3 Ã¢â‚¬â€œ secondary education or 4 Ã¢â‚¬â€œ higher education) 
8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 Ã¢â‚¬â€œ 5th to 9th grade, 3 Ã¢â‚¬â€œ secondary education or 4 Ã¢â‚¬â€œ higher education) 
9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 
10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 
11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') 
12 guardian - student's guardian (nominal: 'mother', 'father' or 'other') 
13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour) 
14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) 
15 failures - number of past class failures (numeric: n if 1<=n<3, else 4) 
16 schoolsup - extra educational support (binary: yes or no) 
17 famsup - family educational support (binary: yes or no) 
18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) 
19 activities - extra-curricular activities (binary: yes or no) 
20 nursery - attended nursery school (binary: yes or no) 
21 higher - wants to take higher education (binary: yes or no) 
22 internet - Internet access at home (binary: yes or no) 
23 romantic - with a romantic relationship (binary: yes or no) 
24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) 
25 freetime - free time after school (numeric: from 1 - very low to 5 - very high) 
26 goout - going out with friends (numeric: from 1 - very low to 5 - very high) 
27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) 
28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) 
29 health - current health status (numeric: from 1 - very bad to 5 - very good) 
30 absences - number of school absences (numeric: from 0 to 93)        "G2"         "grades"  



##part 3

•	Visually clear and interesting explorations of the variables of interest in the data, from the point of view of your research question. Include interpretations of the distributions and relationships of the variables. Use captions to draw the reader’s focus on the interesting parts of your tables and graphics. (max 8 points)

I produced visual and numerical explorations of the variables. 

```{r}
library(FactoMineR)
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
library(GGally)
#modifying the data to observe only the numerical variables
keep_columns <- c("absences", "Dalc","Walc", "goout", "freetime","famrel", "failures", "studytime", "traveltime", "Fedu", "Medu", "age","G3")
alc_lda <- dplyr::select(alc, one_of(keep_columns))
#producing distributions as barplots for all variables
gather(alc_lda) %>% ggplot(aes(value)) + geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust= 1, size = 8)) + facet_wrap("key",scales = "free")
#producing summary statics by grade groups 
alc %>% group_by(sex) %>% summarise(count = n(), mean_grade = mean(G3) )

#producing a correlation matrix of the variables. 
cor_matrix<-cor(alc_lda) %>% round(digits=2)
cor_matrix
```
Most of the variables except for age are,goout,freetime,studytime,famrel not really normally distributed. Male and female average grades are quite same. For correlations there are not many quite obvious correlations. Failures is (-0.37) quite strongly negatively correlated with the finald grade. And not surprisingly alcohol use is correlated with going out. Also there is -0.26 correlation between studytime and weekend alcohol use. Also mother's and father's education levels are correlated strongly with 0.6. Mother's education has a positive correlation with final grade (0.24). 


## Part 3 
Brief description of the method

I am using linear discriminant analysis and linear regression analysis. Linear discriminant analysis is a classification method that finds linear combination to divide the target variable. The target variable can be categorical or binary. The methods is closely linked to other dimensionality reduction methods and also regression analysis. Multiple correspondance analysis is a methodanalyze qualitative data and it is an extension of Correspondence analysis. Correspondance analysis is very similar to principal component analysis but with categorical variables. 

First Linear discriminant analysis
## Part 4_1 
```{r}
library(MASS)
library(dplyr)

#selecting non-nominal variables
keep_columns <- c("absences", "Dalc", "goout", "freetime", "failures", "studytime", "traveltime", "Fedu", "Medu", "age","G3")
alc_lda <- dplyr::select(alc, one_of(keep_columns))
alc_lda

# standardise the variables
scaled_alc <- scale(alc_lda)
#saving scaled_alc as a dataframe
scaled_alc <- as.data.frame(scaled_alc)


#creating categorical variable grades
is.data.frame((scaled_alc))
#saving the scaled grades rate as scaled_grades
scaled_grades <- scaled_alc$G3

bins <- quantile(scaled_grades)
bins
#creating a quantile vector variable "grades"


#creating a categorical variable grades1
grades1 <- cut(scaled_grades, breaks=bins, include.lowest = TRUE, labels = c("low","med_low", "med_high", "high"))
grades1
#removing the orginal variable
scaled_alc <- dplyr::select(scaled_alc, -G3)

#adding the new categorical variable
scaled_alc <- data.frame(scaled_alc, grades1)
glimpse(scaled_alc)
#dividing my data to train and test sets : 
n <- nrow(scaled_alc)
#choosing 80% of the rows 
ind <- sample(n, size = n*0.8)
str(scaled_alc)
#creating the training set 
train <- scaled_alc[ind,]
#creating test set 
test <- scaled_alc[-ind,]



#linear discriminant analysis
lda.fit <- lda(grades1 ~., data = train)

# print the lda.fit object
lda.fit
#Drawing the lda biplot with arrows 
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "purple", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0,
         x1= myscale * heads[,choices[1]],
         y1= myscale * heads[,choices[2]], col = color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col = color, pos = 3)
}

classes <- as.numeric(train$grades)


#plotting the lda
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
```
•	Presentation of the results of your analysis including visualizations and summaries and a thorough interpretation of the results including a validation analysis of the method. (max 16 points)

###4_1 Linear discriminant analysis results and explanations. 

In the LDA the linear discriminant 1 (LD1)  explain 65% of the variance between groups and second only 27.5%. None of the variables present huge impacts on the model with less 1 or bigger than -1 values. Most negative impact on LD1 is due to failures. On LD2 most negative impact is due to daily alcohol use and positive impact from failures and studytime and mother education. 

In the LDA biplot we have the classes in different colour and the variables in the middle as arrows. The direction and length of the arrow displays how the variable impacts the model and these are based on the coefficients. So as the failures  variable coefficient is quite big compared to others (-0.65552574) it is clearly visible in the visualisation as well. 

From the analysis we can gather that Linear discriminant analysis did not yield clear differences between the variables. Now I will continue and test the predictive power of the LDA model with the test data.  

```{r}
#now using the model to predict grades 
#saving the correct variables from the test data
correct_classes <- test$grades1

#removing the grades variable from the test data 
test<- dplyr::select(test, -grades1)

a.pred <- predict(lda.fit, newdata = test)

#cross tabulating the results
table(correct = correct_classes , predicted = lda.pred$class)

```
From the table we can see that the model dow work really well on predicting allocations of new data. The prediction is actually really poor.




## Part 4_2
```{r}
library(tidyr); library(dplyr); library(ggplot2)
grades1


keep <- c("romantic","higher","paid", "famsup", "schoolsup", "guardian", "school", "internet", "nursery", "reason", "Fjob", "Mjob", "Pstatus", "famsize", "address", "sex","scaled_grades" )
alc_MCA <- dplyr::select(alc, one_of(keep))
#adding grades variable to the dataframe 
alc_MCA <- data.frame(alc_MCA, grades1)

#plotting the dataframe 
gather(alc_MCA) %>% ggplot(aes(value)) + geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) + facet_wrap("key", scales = "free") 
str(alc_MCA)
#performing multiple correspondance analysis without plotting it 
mca <- MCA(alc_MCA, graph = FALSE)

# summary of the model
summary(mca)
?MCA
# visualising MCA
plot(mca, invisible=c("ind"),habillage = "quali")


```



## Part 5

•	Conclusions and discussion (max 2 points)

## Part 6
•	An ‘abstract’ at the beginning of the page with a summary of your analysis (max 2 points)
Then I produced the fitted logistic regression model with the variables chosen earlier. 

```{r}

dist_eu <- dist(alc_lda)
#adding the max number of clusters first 15 and then 2 as this was the optimal. 
k_max <- 15
# calculating the total with sum of squares 
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualising the results 
plot(1:k_max, twcss, type='b')

# running the k-means clustering
km2 <-kmeans(dist_eu, centers = 3)

#plotting the scaled_boston dataset with clusters

pairs(alc_lda, col = km2$cluster)
lda.fit <- lda(km2$cluster ~ ., data = alc_lda)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}


plot(lda.fit, dimen = 3, col = km2$cluster, pch = km2$cluster)
lda.arrows(lda.fit, myscale = 1)
```